{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "686eec3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#Data Preprocessing Package\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as sklearn_stop_words\n",
    "import re\n",
    "import nltk\n",
    "#from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.tokenize import WordPunctTokenizer \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as sklearn_stop_words\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "df9f3bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6fa17b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as sklearn_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "65b3891d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4e2674e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8cb0bbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "test=pd.read_excel('./Topic_Modeling/Topic_Modeling/Data/전처리.xlsx')\n",
    "df=test[['filelist','rawdata','cleaned']]\n",
    "df.head()\n",
    "raw = list(df['cleaned'])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79925bc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3b2a15b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './Topic_Modeling/Topic_Modeling/Data/TXT_convert'\n",
    "filelist = os.listdir(directory)\n",
    "df=pd.DataFrame(columns=['filelist','rawdata'])\n",
    "df['filelist']=filelist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2efe7f72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '131.DL E_C_2021.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-4756f93ec47c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'filelist'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mtxt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '131.DL E_C_2021.txt'"
     ]
    }
   ],
   "source": [
    "for i in range(len(df)):\n",
    "    j = df['filelist'][i]\n",
    "    filename = j\n",
    "    f=open(filename, 'r', encoding='utf-8')\n",
    "    txt = f.readlines()\n",
    "    f.close()\n",
    "    df['rawdata'][i] = txt\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "96d94996",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern=re.compile(r'([\\t\\r()-.,:;!?`‘\\'\"�‧|%“])+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3730efb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c680306",
   "metadata": {},
   "outputs": [],
   "source": [
    "#수정버전!!!\n",
    "\n",
    "stop_words=nltk.corpus.stopwords.words('english')\n",
    "stop_words=list(set(stop_words).union(set(sklearn_stop_words)))\n",
    "tokenizer=WordPunctTokenizer()\n",
    "lemma = WordNetLemmatizer()\n",
    "remove_bracket=['step','report','reporting']\n",
    "stop_words.extend(remove_bracket)#\n",
    "len(stop_words)\n",
    "def prepro(text):\n",
    "\n",
    "    a=text.lower()\n",
    "    a=re.sub('\\n',' ',a)\n",
    "    a=re.sub('x0002','',a)\n",
    "    b = re.sub('[^a-zA-Z ]','',a).strip()\n",
    "\n",
    "    tagged_list = pos_tag(tokenizer.tokenize(b))\n",
    "    ad_and_nouns = [t[0] for t in tagged_list if (t[1] == \"NN\")or(t[1]==\"NNP\")]\n",
    "    e_list= [lemma.lemmatize(i) for i in ad_and_nouns]\n",
    "    result=[w for w in e_list if (w not in stop_words) and  (len(w) > 2)]\n",
    "    # remove single character\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6d5543d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "stop_words=nltk.corpus.stopwords.words('english')\n",
    "stop_words=list(set(stop_words).union(set(sklearn_stop_words)))\n",
    "tokenizer=TreebankWordTokenizer()\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "remove_bracket=['[',']','step']\n",
    "stop_words.extend(remove_bracket)\n",
    "len(stop_words)\n",
    "\n",
    "def prepro(text):\n",
    "    a=text.lower()\n",
    "    b=re.sub(pattern,'',a)\n",
    "    b=re.sub(r'\\[\\[(?:[^\\]|]*\\|)?([^\\]|]*)\\]\\]', '', b)\n",
    "    c=re.sub('\\n',' ',b)\n",
    "    tagged_list = pos_tag(tokenizer.tokenize(c))\n",
    "    ad_and_nouns = [t[0] for t in tagged_list if (t[1] == \"NN\")or(t[1]==\"NNP\") or(t[1]=='JJ')]\n",
    "    e_list= [lemma.lemmatize(i) for i in ad_and_nouns]\n",
    "    result=[w for w in e_list if w not in stop_words]\n",
    "#     for w in e_list: \n",
    "#         if w not in stop_words: \n",
    "#             result.append(w)\n",
    "    return result\n",
    "    \n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "403bdc71",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-2d9199f45a4a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mcleaned\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrawdt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mraw_list\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprepro\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrawdt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mraw_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_list\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m#instead of .split()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mcleaned\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_str\u001b[0m\u001b[1;33m)\u001b[0m       \u001b[1;31m#for writing in forms of string\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-20-2d9199f45a4a>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mcleaned\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrawdt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mraw_list\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprepro\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrawdt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mraw_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_list\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m#instead of .split()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mcleaned\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_str\u001b[0m\u001b[1;33m)\u001b[0m       \u001b[1;31m#for writing in forms of string\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-19-f88f2d19c36d>\u001b[0m in \u001b[0;36mprepro\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'[^a-zA-Z ]'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mtagged_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[0mad_and_nouns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtagged_list\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"NN\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;32mor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m\"NNP\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0me_list\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlemma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mad_and_nouns\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Anaconda3\\lib\\site-packages\\nltk\\tag\\__init__.py\u001b[0m in \u001b[0;36mpos_tag\u001b[1;34m(tokens, tagset, lang)\u001b[0m\n\u001b[0;32m    162\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m     \"\"\"\n\u001b[1;32m--> 164\u001b[1;33m     \u001b[0mtagger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_tagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    165\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_pos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Anaconda3\\lib\\site-packages\\nltk\\tag\\__init__.py\u001b[0m in \u001b[0;36m_get_tagger\u001b[1;34m(lang)\u001b[0m\n\u001b[0;32m    104\u001b[0m         \u001b[0mtagger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0map_russian_model_loc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m         \u001b[0mtagger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPerceptronTagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtagger\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Anaconda3\\lib\\site-packages\\nltk\\tag\\perceptron.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, load)\u001b[0m\n\u001b[0;32m    166\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m             AP_MODEL_LOC = \"file:\" + str(\n\u001b[1;32m--> 168\u001b[1;33m                 \u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"taggers/averaged_perceptron_tagger/\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mPICKLE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    169\u001b[0m             )\n\u001b[0;32m    170\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAP_MODEL_LOC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    528\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    529\u001b[0m         \u001b[1;31m# Is the path item a directory or is resource_name an absolute path?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 530\u001b[1;33m         \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mpath_\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    531\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mzipfile\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    532\u001b[0m                 \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl2pathname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Anaconda3\\lib\\genericpath.py\u001b[0m in \u001b[0;36misdir\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;34m\"\"\"Return true if the pathname refers to an existing directory.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m         \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mOSError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "raw=list(df['rawdata'])\n",
    "cleaned=[]\n",
    "for rawdt in raw:\n",
    "    raw_list=[' '. join(prepro(i)) for i in rawdt]\n",
    "    raw_str = ' '.join(raw_list)  #instead of .split()\n",
    "    cleaned.append(raw_str)       #for writing in forms of string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2fbe275e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filelist</th>\n",
       "      <th>rawdata</th>\n",
       "      <th>cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>131.DL E_C_2021.txt</td>\n",
       "      <td>[2021\\n, SUSTAINABILITY\\n, REPORT\\n, \\n, \f",
       "2021...</td>\n",
       "      <td>sustainability report   dl sustainability rep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>135.현대로템_국문_2021.txt</td>\n",
       "      <td>[2021 현대로템 지속가능경영보고서\\n, \\n, Creative Innovatio...</td>\n",
       "      <td>현대로템 지속가능경영보고서  creative innovation future hyu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>136.제일기획_국문_2014-2015.txt</td>\n",
       "      <td>[Ideas\\n, that\\n, Move\\n, 지속가능경영\\n, 보고서\\n, 201...</td>\n",
       "      <td>지속가능경영 보고서      지속가능경영 보고서     ceo message ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>137.BNK금융지주_2020.txt</td>\n",
       "      <td>[BNK Financial Group\\n, Sustainability Report ...</td>\n",
       "      <td>bnk financial group sustainability report  new...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>138.효성_2020.txt</td>\n",
       "      <td>[Sustainability\\n, \\n, with Stakeholders\\n, 20...</td>\n",
       "      <td>sustainability   sustainability report  report...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>코오롱인더19.txt</td>\n",
       "      <td>[\\n, \\n, \\n, \\n, \\n, \\n, \\n, \\n, \\n, Lifestyle...</td>\n",
       "      <td>lifestyle innovator kolon report     ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>한국가스공사.txt</td>\n",
       "      <td>[BETTER ENERGY,\\n, BETTER WORLD\\n, ESG REPORT ...</td>\n",
       "      <td>energy world esg report  neo kogas  kogas glob...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>한화생명.txt</td>\n",
       "      <td>[Hanwha Life\\n, \\n, Sustainability Report 2020...</td>\n",
       "      <td>hanwha life  sustainability report  report ove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>효성첨단소재2020.txt</td>\n",
       "      <td>[Sustainability\\n, \\n, with Stakeholders\\n, 20...</td>\n",
       "      <td>sustainability   hyosung sustainability report...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>효성티앤씨2020.txt</td>\n",
       "      <td>[Sustainability\\n, \\n, with Stakeholders\\n, 20...</td>\n",
       "      <td>sustainability   sustainability report  report...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>114 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      filelist  \\\n",
       "0          131.DL E_C_2021.txt   \n",
       "1         135.현대로템_국문_2021.txt   \n",
       "2    136.제일기획_국문_2014-2015.txt   \n",
       "3         137.BNK금융지주_2020.txt   \n",
       "4              138.효성_2020.txt   \n",
       "..                         ...   \n",
       "109         코오롱인더19.txt   \n",
       "110        한국가스공사.txt   \n",
       "111            한화생명.txt   \n",
       "112    효성첨단소재2020.txt   \n",
       "113       효성티앤씨2020.txt   \n",
       "\n",
       "                                               rawdata  \\\n",
       "0    [2021\\n, SUSTAINABILITY\\n, REPORT\\n, \\n, \n",
       "2021...   \n",
       "1    [2021 현대로템 지속가능경영보고서\\n, \\n, Creative Innovatio...   \n",
       "2    [Ideas\\n, that\\n, Move\\n, 지속가능경영\\n, 보고서\\n, 201...   \n",
       "3    [BNK Financial Group\\n, Sustainability Report ...   \n",
       "4    [Sustainability\\n, \\n, with Stakeholders\\n, 20...   \n",
       "..                                                 ...   \n",
       "109  [\\n, \\n, \\n, \\n, \\n, \\n, \\n, \\n, \\n, Lifestyle...   \n",
       "110  [BETTER ENERGY,\\n, BETTER WORLD\\n, ESG REPORT ...   \n",
       "111  [Hanwha Life\\n, \\n, Sustainability Report 2020...   \n",
       "112  [Sustainability\\n, \\n, with Stakeholders\\n, 20...   \n",
       "113  [Sustainability\\n, \\n, with Stakeholders\\n, 20...   \n",
       "\n",
       "                                               cleaned  \n",
       "0     sustainability report   dl sustainability rep...  \n",
       "1    현대로템 지속가능경영보고서  creative innovation future hyu...  \n",
       "2       지속가능경영 보고서      지속가능경영 보고서     ceo message ...  \n",
       "3    bnk financial group sustainability report  new...  \n",
       "4    sustainability   sustainability report  report...  \n",
       "..                                                 ...  \n",
       "109           lifestyle innovator kolon report     ...  \n",
       "110  energy world esg report  neo kogas  kogas glob...  \n",
       "111  hanwha life  sustainability report  report ove...  \n",
       "112  sustainability   hyosung sustainability report...  \n",
       "113  sustainability   sustainability report  report...  \n",
       "\n",
       "[114 rows x 3 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['cleaned'] =cleaned\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ea24823",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "df.to_pickle('전처리.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "be10813d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('전처리.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9068647a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ebeafcdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6fa9ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9027aef0c98af2431a1a4798d6143261136f5e72e4c0f1a463df25852ac1670e"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
